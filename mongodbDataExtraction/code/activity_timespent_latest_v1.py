"""
Original file from
https://github.com/gnowledge/gstudio/blob/master/gnowsys-ndf/gnowsys_ndf/ndf/management/commands/activity_timespent.py

Modified to work without gstudio / django.

Assumes that activity logs are located in directories called
`gstudio-exported-users-analytics-csvs`.

From conversation with Glenda, to support the Adoption Study, we'll reformat
the output as data after a certain date, with four columns per active date:

* Date of activity
* Number of timestamps (gstudio + tools)
* Total time spent on the given date (oldest - newest)
* Text-list of activities (unique set)
* Bracket-separated list of timestamps per activity

This would be repeated for each date a given student has system activity.

For example, one row might be:

12/1/2017, 5, '06:32:01.612000', PoliceQuad, [12/1/2017 5:01pm, 12/1/2017 5:02pm]
                                 Let's talk  [12/1/2017 10:30am, 12/1/2017 10:35am]
                                 Reflection  [12/1/2017 11:00am]
"""

import csv
import datetime
import json
import os

import pytz

try:
    from bson import ObjectId
except ImportError:  # old pymongo
    from pymongo.objectid import ObjectId

from copy import deepcopy
from diskcache import Cache
from pymongo import MongoClient
from slugify import slugify

from settings import MONGO_DB_PORT
from tool_logs import get_tool_logs, tool_headers


def activity_details(school, start_date, db_path):
    """ Expects a school as a string, like sp100, and a start_date
        as a datetime object """
    if not isinstance(school, basestring):
        raise TypeError('school must be a string')
    if not isinstance(start_date, datetime.datetime):
        raise TypeError('start_date must be a datetime object')
    school = school.lower()

    file_name = school_filename(school)

    # Generate the list of users that have logged in, from the pg_dump_all.sql
    #    file.
    #  1) Look for "COPY auth_user "
    #  2) Start reading rows
    #  3) Match all rows where the last_login != date_joined
    #  4) Stop at the first "--" line
    #
    # Return a list of tuples [(username, userID)]
    user_list = generate_user_list(db_path)
    full_user_list = generate_user_list(db_path, True)

    all_user_data = {}

    for user_data in user_list:
        username = unicode(user_data[0].strip())
        user_id = user_data[1]

        if username not in all_user_data:
            all_user_data[username] = [user_id]

        # activity_in_regex_pattern = '.*/course/activity_player.*'
        # activity_out_regex_pattern = '.*/course.*|.*my-desk.*|.*explore.*|.*tools/tool-page.*|.*course/content.*'

        activity_in_text_search = 'activity_player'
        # activity_out_text_search = 'course my-desk explore tool-page content'

        MC = MongoClient(host='localhost', port=MONGO_DB_PORT)
        GSTUDIO_DB = MC['gstudio-mongodb']

        # This is timing out when there are many records,
        #   so set the `no_cursor_timeout`
        #   optional flag.
        # Changed from $regex to $text, which requires a
        #   $text index on `calling_url` to be set...
        all_visits = GSTUDIO_DB['Benchmarks'].find(
            {
                'last_update': {'$gte': convert_ist_to_utc(start_date)},
                '$text':
                {
                    '$search': activity_in_text_search
                },
                'user': username
            },
            no_cursor_timeout=True
        ).sort('last_update', -1)

        print "\nTotal activity-player visits for {0}: {1}".format(
            username,
            all_visits.count())

        # Once we know the sequence of Activity / Tool interaction,
        #   we need to aggregate the data into the desired format
        # user_row = []  # prepend username and user_id later

        user_nav_out_by_session = get_all_nav_out_events_by_session(username,
                                                                    GSTUDIO_DB)

        for each_visit in all_visits:
            # Temporarily store each of these in row_data.
            # We'll also add in the tool activity to row_data.
            # Then sort by visited_on.
            row_blob = {
                'unit': 'NA',
                'visited_on': 'NA',
                'language': 'NA',
                'lesson': 'NA',
                'activity': 'NA',
                'timespent': 'NA',
                'buddies': 'NA',
                'out_action': 'NA',
                '_type': 'activity'
            }

            # last_update is saved as IST via Django, even though
            #   it looks like UTC in MongoDB:
            #    ISODate("2017-06-18T04:49:46.243Z")
            # Because they create the Benchmark last_updated field
            #    with `datetime.datetime.now()`, which is system time.
            # So to make it consistent with the tool logging, we'll
            #   convert it to IST...
            visited_on = convert_utc_to_ist(each_visit['last_update'])

            # if visited_on.date() >= start_date.date():
            row_blob['visited_on'] = visited_on
            locale = 'en'
            if 'locale' in each_visit:
                locale = each_visit['locale']

            row_blob['language'] = str(locale)
            calling_url_str = each_visit['calling_url']
            if (calling_url_str.startswith('/') and
                    calling_url_str.endswith('/')):
                splitted_results = calling_url_str.split('/')

                if len(splitted_results) == 7:
                    unit_id = splitted_results[1]
                    lesson_id = splitted_results[4]
                    activity_id = splitted_results[5]
                    unit_node = get_group_name_id(unit_id, get_obj=True)
                    lesson_node = GSTUDIO_DB['Nodes'].find_one(
                        {'_id': ObjectId(lesson_id)})
                    activity_node = GSTUDIO_DB['Nodes'].find_one(
                        {'_id': ObjectId(activity_id)})
                    if not lesson_node:
                        lesson_name = 'Deleted lesson? Unknown name.'
                    else:
                        lesson_name = lesson_node['name']
                    if not activity_node:
                        activity_name = 'Deleted activity? Unknown name.'
                    else:
                        activity_name = activity_node['name']
                    if not unit_node:
                        unit_name = 'Deleted unit? Unknown name.'
                    else:
                        unit_name = unit_node['name']
                        if 'altnames' in unit_node and unit_node['altnames']:
                            unit_name = unit_node['altnames']
                    row_blob.update(
                        {'unit': slugify(unit_name),
                         'lesson': slugify(lesson_name),
                         'activity': slugify(activity_name)})

                    # Using $regex is slow and kills performance...
                    #   4-5 seconds per query.
                    # Wonder how we can speed it up while preserving
                    #   the data captured.

                    # Changed from $regex to $text, which requires a
                    #   $text index on `calling_url` to be set...
                    # Manually checking, it seems to capture the same data.
                    # nav_out_action_cur = GSTUDIO_DB['Benchmarks'].find(
                    #     {'last_update': {'$gte': each_visit['last_update']},
                    #      '_id': {'$ne': each_visit['_id']},
                    #      'user': username,
                    #      'session_key': each_visit['session_key'],
                    #      '$text': {
                    #          '$search': activity_out_text_search
                    #     }}, {'last_update': 1,
                    #          'name': 1}).sort('last_update', 1).limit(1)

                    nav_out_action_cur = find_next_nav_out_action(
                        user_nav_out_by_session,
                        each_visit
                    )
                    if nav_out_action_cur is not None:
                        # if nav_out_action_cur.count():
                        # nav_out_obj = nav_out_action_cur[0]
                        nav_out_obj = nav_out_action_cur
                        end_time = convert_utc_to_ist(
                            nav_out_obj['last_update'])
                        timespent = (end_time - visited_on).total_seconds()
                        print " Time spent: ", timespent, " seconds."
                        row_blob.update({
                            'timespent': str(timespent),
                            'out_action': nav_out_obj['name']})
                    else:
                        print (" ## Unable to track time spent "
                               "on this activity. ##")
                    # Get the buddy ID from the username, using the CSVs
                    buddies_obj = GSTUDIO_DB['Buddies'].find_one({
                        'loggedin_userid': int(user_id),
                        'session_key': str(each_visit['session_key'])
                    })
                    if buddies_obj:
                        auth_id_list = buddies_obj['buddy_in_out'].keys()
                        buddies_names = get_names_list_from_obj_id_list(
                            auth_id_list, u'Author')
                        row_blob.update({'buddies': buddies_names})
                        for buddy in buddies_names:
                            if buddy not in all_user_data:
                                # how do we get the buddy's user ID here?
                                all_user_data[buddy] = [get_buddy_id(
                                    full_user_list,
                                    buddy)]
                            all_user_data[buddy].append(row_blob)
                else:
                    print ("## Unable to track time spent "
                           "on this activity. ##")
                all_user_data[username].append(row_blob)
        all_visits.close()

    # Grab tool log data
    for username, user_row in all_user_data.items():
        user_id = user_row[0]

        # Now also check for tool logs
        # Do this here because buddies may not have ``all_visits``
        #   from above, so they won't get their log files checked if
        #   we leave this log-check above.
        for directory, subdirectory, files in os.walk(db_path):
            if ('gstudio_tools_logs' in directory and
                    user_log_file_in(user_id, files)):
                full_path = os.path.join(directory,
                                         get_user_log_filename(user_id, files))
                user_row += get_tool_logs(full_path, start_date)
                # Don't need to do this check for buddies, because gstudio
                #   seems to log each buddy's tool events separately.

    # Once we've collected all the user data + logs, go through
    #   each one and sort them.
    # Grab all of the found dates, so that we can add
    #   filler data, so each user row in the output
    #   has columns that line up.
    all_dates = get_all_dates_from_data(all_user_data)

    # Keep in mind that index 0 is the user_id
    row_data = []
    for username, user_row in all_user_data.items():
        user_id = user_row[0]
        del user_row[0]

        # Now all the data is in user_row, let's sort them in ascending
        #   order by the `visited_on` key.
        user_row = sorted(user_row, key=lambda k: k['visited_on'])

        # Aggregate the data into five columns per date
        user_row = aggregate_user_data(user_row, all_dates)

        # We need to convert the datetime objects to nice strings
        # user_row = [serialize_datetime(r) for r in user_row]

        # Prepend the username and userID to the row
        # user_row = [username, user_id] + [data for r in user_row
        #                                   for data in extract_data(r)]
        user_row = [username, user_id] + user_row
        row_data.append(user_row)

    with open(file_name, 'a') as file_handle:
        activity_writer = csv.writer(file_handle,
                                     dialect='excel')

        column_list = ['Username', 'UserID']

        for one_date in all_dates:
            column_list += ['Date ({0})'.format(str(one_date)),
                            'Total Timestamps',
                            'Total Timespent (HH:MM:SS.microsec)',
                            'Activities Viewed',
                            'Activity Timestamps']
        activity_writer.writerow(column_list)
        # write the row_data out to the activity_writer
        for row in row_data:
            activity_writer.writerow(row)
    return len(row_data)


def activity_headers():
    """ return the standardized set of activity headers for each in / out
        action """
    return ['Unit', 'VisitedOn (IST)',
            'Language', 'Lesson', 'Activity',
            'Timespent', 'Buddies', 'OutAction']


def aggregate_user_data(user_row, all_dates):
    """
    From conversation with Glenda, to support the Adoption Study, we'll
    reformat the output as data within a date range. `all_dates` should
    include all student dates, which means that this student most likely
    will not have activity on some of those dates. Blank cells are
    inserted for these dates.

    For dates with data, we'll have five columns per
    active date:

    * Date of activity
    * Number of timestamps (gstudio + tools)
    * Total time spent on the given date (oldest - newest)
    * Text-list of activities (unique set)
    * Bracket-separated list of timestamps per activity

    This would be repeated for each date a given student has system activity.

    For example, one row might be:

    12/1/2017, 5, '06:32:01.612000', PoliceQuad, [12/1/2017 5:01pm, 12/1/2017 5:02pm]
                                     Let's talk  [12/1/2017 10:30am, 12/1/2017 10:35am]
                                     Reflection  [12/1/2017 11:00am]


    """
    results = []
    student_unique_dates = filter_unique_dates(user_row)
    for date in all_dates:
        if date in student_unique_dates:
            activity_count = count_events_by_date(user_row, date)
            activity_list = get_activities_by_date(user_row, date)
            total_activity_time = get_total_activity_time(user_row, date)
            activity_timestamps = get_activity_timestamps_by_date(user_row, date)
            results.append(get_timestamp_as_date_string(date))
            results.append(activity_count)
            results.append(total_activity_time)
            results.append(activity_list)
            results.append(activity_timestamps)
        else:
            results += [' '] * 5
    return results


def convert_ist_to_utc(ist_time):
    """
    last_update is saved as IST via Django, even though
      it looks like UTC in MongoDB:
       ISODate("2017-06-18T04:49:46.243Z")
    Because they create the Benchmark last_updated field
       with `datetime.datetime.now()`, which is system time (IST).
    So in order to do a $gte filter against the user input
        `start_date`, we need to convert `start_date` to UTC.
    """
    return ist_time.replace(tzinfo=pytz.UTC)


def convert_utc_to_ist(utc_time):
    """
    last_update is saved as IST via Django, even though
      it looks like UTC in MongoDB:
       ISODate("2017-06-18T04:49:46.243Z")
    Because they create the Benchmark last_updated field
       with `datetime.datetime.now()`, which is system time (IST).
    So to make it consistent with the tool logging, we'll
      convert this "fake" UTC time to actual IST...
    """
    ist = pytz.timezone('Asia/Kolkata')
    return utc_time.replace(tzinfo=ist)


def count_events_by_date(user_row, date):
    """ Given a specific date object, count how many timestamps appear
    in the user data on that date """
    return len(filter_activities_by_date(user_row, date))


def extract_data(blob):
    """ pull out only the values we want saved, in order of the headers """
    if blob['_type'] == 'activity':
        return [blob['unit'],
                blob['visited_on'],
                blob['language'],
                blob['lesson'],
                blob['activity'],
                blob['timespent'],
                blob['buddies'],
                blob['out_action']]
    elif blob['_type'] == 'tool':
        return [blob['app_name'],
                blob['created_at'],
                blob['event_type'],
                blob['params']]
    return []


def extract_unique_activity_names(activities):
    """ return just the names of all activities.
    To make them appear semi-logically, sort them according to
    the first timestamp in each activity's timestamps... """
    unique_names = []
    sorted_activities = sorted(activities, key=lambda k: k['visited_on'])
    for activity in sorted_activities:
        activity_name = extract_name(activity)
        if activity_name not in unique_names:
            unique_names.append(activity_name)
    return unique_names


def extract_name(blob):
    """ pull out the activity name to show in the cell """
    if blob['_type'] == 'activity':
        return '{0} / {1} / {2}'.format(blob['unit'],
                                        blob['lesson'],
                                        blob['activity'])
    elif blob['_type'] == 'tool':
        if 'app_name' in blob:
            return blob['app_name']
        elif 'appName' in blob:
            return blob['appName']
        return 'Unknown Tool??'
    return None


def filter_activities_by_date(user_row, date):
    """ return all the activities on a specific date """
    return [blob for blob in user_row
            if blob['visited_on'].date() == date]


def filter_unique_dates(user_row):
    """ given a list of activities (all with `visited_on` as a datetime
    object), returns the list of unique DATES """
    all_dates = [blob['visited_on'].date() for blob in user_row]
    return sorted(list(set(all_dates)))


def find_next_nav_out_action(user_nav_out_by_session, each_visit):
    """ find the next event, hopefully like

    nav_out_action_cur = GSTUDIO_DB['Benchmarks'].find(
        {'last_update': {'$gte': each_visit['last_update']},
         '_id': {'$ne': each_visit['_id']},
         'user': username,
         'session_key': each_visit['session_key'],
         '$text': {
             '$search': activity_out_text_search
        }}, {'last_update': 1,
             'name': 1}).sort('last_update', 1).limit(1)
    """
    session_key = each_visit['session_key']
    visit_id = each_visit['_id']
    visit_time = each_visit['last_update']
    match = None

    for visit in user_nav_out_by_session[session_key]:
        if visit['last_update'] >= visit_time and visit['_id'] != visit_id:
            return visit

    return match


def get_all_dates_from_data(all_user_data):
    """ Get all of the dates in the dataset, across all users """
    all_dates = []
    for username, row in all_user_data.items():
        # Avoid the [0] index, which is `user_id`
        all_dates += filter_unique_dates(row[1::])
    return sorted(list(set(all_dates)))


def get_all_nav_out_events_by_session(username, GSTUDIO_DB):
    """ see if getting all the user events once and putting
    them into memory as a dict with session_key as the keys,
    makes things faster instead of re-querying MongoDB
    constantly. Need to be able to replicate:

    nav_out_action_cur = GSTUDIO_DB['Benchmarks'].find(
        {'last_update': {'$gte': each_visit['last_update']},
         '_id': {'$ne': each_visit['_id']},
         'user': username,
         'session_key': each_visit['session_key'],
         '$text': {
             '$search': activity_out_text_search
        }}, {'last_update': 1,
             'name': 1}).sort('last_update', 1).limit(1)

    More or less...
    """
    activity_out_text_search = 'course my-desk explore tool-page content'

    all_out_records = GSTUDIO_DB['Benchmarks'].find(
        {
            'user': username,
            '$text': {
                '$search': activity_out_text_search
            }
        }, {
            'last_update': 1,
            'name': 1,
            'session_key': 1,
            '_id': 1}).sort('last_update', 1)
    results = {}
    for record in all_out_records:
        session_key = record['session_key']
        if session_key not in results.keys():
            results[session_key] = []
        results[session_key].append(record)

    # now sort all records by `last_update`
    for session, records in results.items():
        results[session] = sorted(records, key=lambda k: k['last_update'])

    return results


def get_buddy_id(user_list, buddy_name):
    """ given a buddy name, return their ID from the list of active users """
    for user_data in user_list:
        username = unicode(user_data[0].strip())
        user_id = user_data[1]
        if username == buddy_name:
            return user_id
    raise LookupError('Buddy not found??')


def get_total_activity_time(user_row, date):
    """ for a given date return the timespan of activity data """
    activities = filter_activities_by_date(user_row, date)
    sorted_activities = sorted(activities, key=lambda k: k['visited_on'])
    time_spent = (sorted_activities[-1]['visited_on'] -
                  sorted_activities[0]['visited_on'])
    return str(time_spent)


def get_group_name_id(group_name_or_id, get_obj=False):
    '''
      Taken from https://github.com/gnowledge/gstudio/blob/master/
        gnowsys-ndf/gnowsys_ndf/ndf/views/methods.py

      - This method takes possible group name/id as an argument and returns
        (group-name and id) or group object.
      - If no second argument is passed, as method name suggests, returned
        result is "group_name" first and "group_id" second.
      - When we need the entire group object, just pass second argument as
        boolian) True. In the case group object will be returned.
      Example 1: res_group_name, res_group_id = get_group_name_id(
        group_name_or_id)
      - "res_group_name" will contain name of the group.
      - "res_group_id" will contain _id/ObjectId of the group.
      Example 2: res_group_obj = get_group_name_id(group_name_or_id,
                                                   get_obj=True)
      - "res_group_obj" will contain entire object.
      Optimization Tip: before calling this method, try to cast group_id to
      ObjectId as follows (or copy paste following snippet at start of function
      or wherever there is a need):
      try:
          group_id = ObjectId(group_id)
      except:
          group_name, group_id = get_group_name_id(group_id)
    '''
    cache = Cache('/tmp/clix-research-data')
    # if cached result exists return it
    if not get_obj:
        slug = slugify(group_name_or_id)
        # for unicode strings like hindi-text slugify doesn't works
        cache_key = 'get_group_name_id_' + str(slug) if slug else str(
            abs(hash(group_name_or_id)))
        cache_result = cache.get(cache_key)

        if cache_result:
            return (cache_result[0], ObjectId(cache_result[1]))
    # ---------------------------------

    MC = MongoClient(host='localhost', port=MONGO_DB_PORT)
    GSTUDIO_DB = MC['gstudio-mongodb']

    # case-1: argument - "group_name_or_id" is ObjectId
    if ObjectId.is_valid(group_name_or_id):

        group_obj = GSTUDIO_DB['Nodes'].find_one(
            {"_id": ObjectId(group_name_or_id)})

        # checking if group_obj is valid
        if group_obj:
            # if (group_name_or_id == group_obj._id):
            group_id = group_name_or_id
            group_name = group_obj['name']

            if get_obj:
                return group_obj
            else:
                # setting cache with both ObjectId and group_name
                cache.set(cache_key, (group_name, group_id), 60 * 60)
                cache_key = u'get_group_name_id_' + slugify(group_name)
                cache.set(cache_key, (group_name, group_id), 60 * 60)
                return group_name, group_id

    # case-2: argument - "group_name_or_id" is group name
    else:
        group_obj = GSTUDIO_DB['Nodes'].find_one(
            {"_type": {"$in": ["Group", "Author"]},
             "name": unicode(group_name_or_id)})

        # checking if group_obj is valid
        if group_obj:
            # if (group_name_or_id == group_obj.name):
            group_name = group_name_or_id
            group_id = group_obj['_id']

            if get_obj:
                return group_obj
            else:
                # setting cache with both ObjectId and group_name
                cache.set(cache_key, (group_name, group_id), 60 * 60)
                cache_key = u'get_group_name_id_' + slugify(group_name)
                cache.set(cache_key, (group_name, group_id), 60 * 60)
                return group_name, group_id

    if get_obj:
        return None
    else:
        return None, None


def get_names_list_from_obj_id_list(obj_ids_list, node_type):
    """ Taken from https://github.com/gnowledge/gstudio/blob/master/gnowsys-ndf/gnowsys_ndf/ndf/models/node.py
    """
    MC = MongoClient(host='localhost', port=MONGO_DB_PORT)
    GSTUDIO_DB = MC['gstudio-mongodb']
    obj_ids_list = map(ObjectId, obj_ids_list)
    nodes_cur = GSTUDIO_DB['Nodes'].find(
        {
            '_type': node_type,
            '_id': {'$in': obj_ids_list}
        },
        {
            'name': 1
        }
    )
    result_list = [node['name'] for node in nodes_cur]
    return result_list


def generate_user_list(db_path, full_list=False):
    # type: (object, object) -> object
    """
    Generate the list of users that have logged in, from the pg_dump_all.sql
       file.
     1) Look for "COPY auth_user "
     2) Start reading rows
     3) Match all rows where the last_login != date_joined
        - Rows are tab delimited
        - last_login = index 2 (starting at 0)
        - date_joined = index 10 (starting at 0)
     4) Stop at the first "\." line

    Return a list of tuples [(username, userID)]
    """
    postgres_dump = os.path.join(db_path, 'postgres-dump', 'pg_dump_all.sql')
    user_list = []
    with open(postgres_dump, 'r') as postgres_file:
        start_reading = False
        for row in postgres_file.readlines():
            if start_reading and row.strip() == '\.':
                # delimiter for end of the COPY block
                break
            if start_reading:
                last_login = grab_last_login(row)
                date_joined = grab_date_joined(row)
                if last_login != date_joined or full_list:
                    user_list.append((grab_username(row),
                                      grab_user_id(row)))
            if row.startswith('COPY auth_user ('):
                start_reading = True

    print('Found {0} active users at this school'.format(str(len(user_list))))
    return user_list


def get_activities_by_date(user_row, date):
    """ aggregate all of the activities that happened on a single date,
    into a single "cell" -- text separated by newlines. """
    activities = filter_activities_by_date(user_row, date)
    activity_names = extract_unique_activity_names(activities)
    return newline_join(activity_names)


def get_activity_timestamps_by_date(user_row, date):
    """ aggregate all of the activity timestamps that happened on a single
    date, into a single "cell" -- bracketed lists separated by newlines """
    activities = filter_activities_by_date(user_row, date)
    # need to make sure it's in the same order as get_activities_by_date
    activity_names = extract_unique_activity_names(activities)
    results = []
    for activity_name in activity_names:
        activity_timestamps = [serialize_datetime(blob)['visited_on']
                               for blob in activities
                               if extract_name(blob) == activity_name]
        results.append(json.dumps(sorted(activity_timestamps)))
    return newline_join(results)


def grab_date_joined(row):
    """ return date_joined, which should be index 10 ... and strip() it
        because it's the last item in the row and includes a \n """
    return row.split('\t')[10].strip()


def grab_last_login(row):
    """ return last_login, which should be index 2 """
    return row.split('\t')[2]


def grab_username(row):
    """ return username, which should be index 4 """
    return row.split('\t')[4]


def grab_user_id(row):
    """ return the ID, which should be index 0 """
    return row.split('\t')[0]


def headers(blob):
    """ if is an activity blob, return the activity headers
        if is a tool blob, return the tool headers """
    if blob['_type'] == 'activity':
        return activity_headers()
    return tool_headers()


def newline_join(stuff):
    """ return a newline joined string, from the stuff list """
    return '\r\n'.join(stuff)


def school_filename(school):
    """ helper method that returns the output file name """
    return '{0}-activity-data.csv'.format(school)


def serialize_datetime(blob):
    """ Make the output human readable,
    ISODate("2017-06-18T04:49:46.243Z") => 2017-06-18  4:27:00 PM """
    blob = deepcopy(blob)
    output_format = '%Y-%m-%d %I:%M:%S %p'
    if blob['_type'] == 'activity':
        blob['visited_on'] = blob['visited_on'].strftime(output_format)
    elif blob['_type'] == 'tool':
        blob['created_at'] = blob['created_at'].strftime(output_format)
        blob['visited_on'] = blob['visited_on'].strftime(output_format)
    return blob


def get_timestamp_as_date_string(datetime_object):
    """ Make the output human readable,
    ISODate("2017-06-18T04:49:46.243Z") => 2017-06-18 """
    output_format = '%Y-%m-%d'
    date_string = datetime_object.strftime(output_format)
    return date_string


def get_user_log_filename(user_id, files):
    """ return the actual user log filename """
    if not user_log_file_in(user_id, files):
        raise IOError('User log not found.')
    if '{0}.json'.format(user_id) in files:
        return '{0}.json'.format(user_id)
    for filename in files:
        if (filename.startswith('{0}-'.format(user_id)) and
                filename.endswith('.json')):
            return filename


def user_log_file_in(user_id, files):
    """ check if `{user_id}-{activity-name}.json` is in the list of files.
    Returns the filename """
    if '{0}.json'.format(user_id) in files:
        return True
    for filename in files:
        if (filename.startswith('{0}-'.format(user_id)) and
                filename.endswith('.json')):
            return True
    return False
